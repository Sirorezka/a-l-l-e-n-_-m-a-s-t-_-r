{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all wiki articles to support word2vec machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get wiki docs:\n",
    "wiki_docs_dir = '../data/wiki_data'\n",
    "\n",
    "def read_wiki_docs_text (dir_data):\n",
    "    docs = []\n",
    "\n",
    "    for fname in os.listdir(dir_data):\n",
    "        path = os.path.join(dir_data, fname)\n",
    "        words = ipyth_utils.tokenize(open(path).read())\n",
    "        docs.append(words)\n",
    "        \n",
    "    return docs\n",
    "\n",
    "wiki_text_data = read_wiki_docs_text (wiki_docs_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['papua', 'earthquake', 'occurred', 'moment', 'magnitude', 'january', 'local', 'time', 'killing', 'least', 'four', 'injuring', 'dozens', 'people', 'epicenter', 'kilometres', 'mi', 'west', 'northwest', 'manokwari', 'km', 'mi', 'east', 'northeast', 'sorong', 'indonesia', 'west', 'papua', 'province', 'bird', 'head', 'peninsula', 'official', 'world', 'vision', 'said', 'ten', 'buildings', 'totally', 'destroyed', 'including', 'several', 'hotels', 'house', 'government', 'official', 'officials', 'said', 'three', 'people', 'staying', 'mutiara', 'hotel', 'city', 'manokwari', 'pulled', 'alive', 'rubble', 'taken', 'hospital', 'two', 'hotels', 'collapsed', 'quake', 'twenty', 'three', 'aftershocks', 'magnitude', 'one', 'magnitude', 'occurring', 'local', 'time', 'january', 'utc', 'another', 'magnitude', 'earthquake', 'also', 'felt', 'nearby', 'papua', 'new', 'guinea', 'darwin', 'australia', 'see', 'also', 'west', 'papua', 'earthquake', 'papua', 'earthquake', 'references', 'external', 'links', 'earthquake', 'papua', 'indonesia', 'nasa', 'earth', 'observatory']\n",
      "\n",
      "\n",
      "total number of docs:  2140\n"
     ]
    }
   ],
   "source": [
    "## Text Example:\n",
    "print(wiki_text_data[0])\n",
    "print (\"\\n\")\n",
    "print (\"total number of docs: \", len (wiki_text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model trained and saved. Time passed in minutes:  0.54\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "#downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "print (\"Training model...\")\n",
    "start_time = time.time()\n",
    "wiki_word2vecmodel = word2vec.Word2Vec(wiki_text_data, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "wiki_word2vecmodel.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "wiki_word2vecmodel.save(model_name)\n",
    "print (\"Model trained and saved. Time passed in minutes: \",round((time.time() - start_time)/60,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loadin train data\n",
    "fname_str = 'joined_set.tsv'\n",
    "\n",
    "data = pd.read_csv('../data/' + fname_str, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When athletes begin to exercise, their heart rates and respiration rates increase.  At what level of organization does the human body coordinate these functions? at the tissue level at the organ level at the system level at the cellular level\n",
      "\n",
      "\n",
      "Model view:  Word2Vec(vocab=76874, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "## Evaluating model\n",
    "res, prob_score = ipyth_word2_vec.predict_cosine_answers (data, wiki_word2vecmodel,num_features)\n",
    "\n",
    "y = data.iloc[0:2500,6]\n",
    "y_predict = res[0:2500]\n",
    "evaluate_score(y_predict, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33679999999999999"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.iloc[0:2500,6]\n",
    "y_predict = res[0:2500]\n",
    "evaluate_score(y_predict,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to fine tune all parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "\n",
    "par_range  = np.arange (5,20,1)\n",
    "scores = []\n",
    "\n",
    "for par_iter in par_range:\n",
    "    print (\"Training model...\")\n",
    "    start_time = time.time()\n",
    "    wiki_word2vecmodel = word2vec.Word2Vec(wiki_text_data, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = par_iter)\n",
    "    \n",
    "    wiki_word2vecmodel.init_sims(replace=True)\n",
    "    y = data.iloc[0:2500,6]\n",
    "    y_predict = res[0:2500]\n",
    "    scores.append(evaluate_score(y_predict, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(par_range,scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
